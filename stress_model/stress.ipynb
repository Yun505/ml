{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The csv is from kaggle\n",
        "\n"
      ],
      "metadata": {
        "id": "MinLFHpqUipR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfs7Di9oS7tf",
        "outputId": "9adc2bac-a3a7-402f-9c76-17e815af3060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          subreddit post_id sentence_range  \\\n",
            "0              ptsd  8601tu       (15, 20)   \n",
            "1        assistance  8lbrx9         (0, 5)   \n",
            "2              ptsd  9ch1zh       (15, 20)   \n",
            "3     relationships  7rorpp        [5, 10]   \n",
            "4  survivorsofabuse  9p2gbc         [0, 5]   \n",
            "\n",
            "                                                text     id  label  \\\n",
            "0  He said he had not felt that way before, sugge...  33181      1   \n",
            "1  Hey there r/assistance, Not sure if this is th...   2606      0   \n",
            "2  My mom then hit me with the newspaper and it s...  38816      1   \n",
            "3  until i met my new boyfriend, he is amazing, h...    239      1   \n",
            "4  October is Domestic Violence Awareness Month a...   1421      1   \n",
            "\n",
            "   confidence  social_timestamp  social_karma  syntax_ari  ...  \\\n",
            "0         0.8        1521614353             5    1.806818  ...   \n",
            "1         1.0        1527009817             4    9.429737  ...   \n",
            "2         0.8        1535935605             2    7.769821  ...   \n",
            "3         0.6        1516429555             0    2.667798  ...   \n",
            "4         0.8        1539809005            24    7.554238  ...   \n",
            "\n",
            "   lex_dal_min_pleasantness  lex_dal_min_activation  lex_dal_min_imagery  \\\n",
            "0                     1.000                  1.1250                  1.0   \n",
            "1                     1.125                  1.0000                  1.0   \n",
            "2                     1.000                  1.1429                  1.0   \n",
            "3                     1.000                  1.1250                  1.0   \n",
            "4                     1.000                  1.1250                  1.0   \n",
            "\n",
            "   lex_dal_avg_activation  lex_dal_avg_imagery  lex_dal_avg_pleasantness  \\\n",
            "0                 1.77000              1.52211                   1.89556   \n",
            "1                 1.69586              1.62045                   1.88919   \n",
            "2                 1.83088              1.58108                   1.85828   \n",
            "3                 1.75356              1.52114                   1.98848   \n",
            "4                 1.77644              1.64872                   1.81456   \n",
            "\n",
            "   social_upvote_ratio  social_num_comments  syntax_fk_grade  sentiment  \n",
            "0                 0.86                    1         3.253573  -0.002742  \n",
            "1                 0.65                    2         8.828316   0.292857  \n",
            "2                 0.67                    0         7.841667   0.011894  \n",
            "3                 0.50                    5         4.104027   0.141671  \n",
            "4                 1.00                    1         7.910952  -0.204167  \n",
            "\n",
            "[5 rows x 116 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = pd.read_csv(\"stress.csv\")\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.isnull().sum()) #There are no null values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci860ngsYOc7",
        "outputId": "6403fa12-6cd0-42b8-8119-0e8e91706611"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subreddit                   0\n",
            "post_id                     0\n",
            "sentence_range              0\n",
            "text                        0\n",
            "id                          0\n",
            "                           ..\n",
            "lex_dal_avg_pleasantness    0\n",
            "social_upvote_ratio         0\n",
            "social_num_comments         0\n",
            "syntax_fk_grade             0\n",
            "sentiment                   0\n",
            "Length: 116, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "stemmer = nltk.SnowballStemmer(\"english\")\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "stopword=set(stopwords.words('english'))\n",
        "\n",
        "def clean(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) #It returns a string with the non-alphanumeric characters escaped by backslashes.\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = [word for word in text.split(' ') if word not in stopword]\n",
        "    text=\" \".join(text)\n",
        "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
        "    text=\" \".join(text)\n",
        "    return text\n",
        "data[\"text\"] = data[\"text\"].apply(clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQaV8UhqYTxR",
        "outputId": "6dc895fc-5ad6-4dec-b908-84c429209fc0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"label\"] = data[\"label\"].map({0: \"No Stress\", 1: \"Stress\"})\n",
        "data = data[[\"text\", \"label\"]]\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMrqWy8G2pwv",
        "outputId": "3c324d8b-2ace-4b16-951b-7b650a0d085c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text      label\n",
            "0  said felt way sugget go rest trigger ahead you...     Stress\n",
            "1  hey rassist sure right place post goe  im curr...  No Stress\n",
            "2  mom hit newspap shock would know dont like pla...     Stress\n",
            "3  met new boyfriend amaz kind sweet good student...     Stress\n",
            "4  octob domest violenc awar month domest violenc...     Stress\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x = np.array(data[\"text\"])\n",
        "y = np.array(data[\"label\"])\n",
        "\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(x)\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(X, y,  test_size=0.5, random_state=42)\n",
        "\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "model = BernoulliNB()\n",
        "model.fit(xtrain, ytrain)\n",
        "\n",
        "user = input(\"Enter a Text: \")\n",
        "data = cv.transform([user]).toarray()\n",
        "output = model.predict(data)\n",
        "print(output)\n",
        "\n",
        "\n",
        "user = input(\"Enter a Text: \")\n",
        "data = cv.transform([user]).toarray()\n",
        "output = model.predict(data)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLz76p8U3dZJ",
        "outputId": "b95ee899-b854-4cf6-daf1-c1f7d5f2ba75"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a Text: i am stressed\n",
            "['No Stress']\n",
            "Enter a Text: STRESS\n",
            "['Stress']\n"
          ]
        }
      ]
    }
  ]
}